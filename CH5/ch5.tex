\documentclass[12pt]{article}
\usepackage{amsmath, bm, amssymb}
\usepackage{tikz, pgfplots}
\pgfplotsset{compat=1.17}
\usepackage[a4paper,bindingoffset=0.2in,left=1in,right=1in,top=1in,bottom=1in,footskip=.25in]{geometry}

\begin{document}
\setlength{\abovedisplayskip}{0pt}
\setlength{\belowdisplayskip}{0pt}
\setlength{\abovedisplayshortskip}{0pt}
\setlength{\belowdisplayshortskip}{0pt}

\title{Notes for ECE269 - Linear Algebra \\
\large Chapter 4}
\author{Sam Cowin}
\maketitle

\section{Linear Equations in Linear Algebra}
\section{Matrix Algebra}
\section{Determinants}
\section{Vector Spaces}
\section{Eigenvalues and Eigenvectors}
Eigenvalues and Eigenvectors appear in many systems, but concerning engineering, the appear in differential equations and continuous dynamical systems. These systems are another way to refer
to difference equations. The dynamical system describes the lienar transformations from one state to the next state, and the Eigenvalues and Eigenvectors help visualize and disect these 
transformations. 
\subsection{Eigenvectors and Eigenvalues}
An eigenvector of an \textit{n x n} matrix A is a nonzero vector \textbf{x} such that $A\mathbf{x}=\lambda\mathbf{x}$ for some scalar $\lambda$. A scalar $\lambda$ is called an eigenvalue of 
A if there is a nontrivial solution \textbf{x} of A\textbf{x}=$\lambda$\textbf{x}; such an \textbf{x} is called an eigenvector corresponding to $\lambda$. $\lambda$ is a eigenvalue of 
the \textit{n x n} matrix A if and only if there is a nontrivial solution to the equation $(A-\lambda I)\mathbf{x=0}$. The eigenspace is a subspace of the null space. Finding a basis for 
the eigenspace is the equivalent of solving the homogeneous equation just described and finding the vector equation corresponding to that solution. while this method works for finding the 
eigenvectors, the reduced echelon form does not showcase the eigenvalues. For there to be a nontrivial solution, to restate there needs to be linear dependence among the columns or free 
variables. One such case where eigenvalues can be found precisely is when you have a triangular matrix and the eigenvalues are the entries of the main diagonol. This is the 
case for the lower triangular matrices as well and repeats are treated as one eigenvalue. Zero can only be an eigenvalue of a matrix if that matrix is not invertible. If there is a set 
of eigenvectors that each correspond to a distinct eigenvalue, then these vectors are linearly independent. One simple way to solve the difference equations is to replace the 
previous state multiplied with the matrix A by the matrix A multiplied with the initial state eigenvector multiplied with eigenvalue of this state to the previous states power. The 
transformation matrix raised to any power multiplied with \textbf{x} is equivalent to the eigenvalue raised to the same power multiplied with \textbf{x}. Multiples of eigenvalues are 
eigenvalues for the same scalar multiple of the matrix that the original eigenvalue was an eigenvalue for. 
\subsection{The Characteristic Equation}
To find the eigenvalues for a given matrix, you must find $\lambda$ values that will make the matrix not invertible. This is done by finding $\lambda$ values that make the following true:
\newline
$$
det(A-\lambda I=0)
$$
\newline
Finding the roots of this equation yields the eigenvalues. This transformed the equation with two unknown in $\lambda$ and \textbf{x} into one with only one unknown in $\lambda$. To 
summarize previous findings, the determinant can be determined for higher order matrices by reducing to echelon form the matrix, and then multiplying -1 to the power of row interchanges 
needed by the pivot position values. If this value is 0, the matrix is not invertible. A scalar satisfying the above equation solves the characteristic equation. The degree of the 
characteristic equation is equivalent to the size of the matrix for an \textit{n x n} matrix. 
\newline
\newline
A matrix A is similar to the matrix B if there is an invertible matrix P such that $P^{-1}AP=B$ and this is referred to as the similarity transformation. If one matrix A and another B are 
similar, then they have the same characteristic polynomial and thus the same eigenvalues with the same multiplicities. Row operations normally change eigenvalues, so row equivalent 
matrices are not similar. Additionally, matrices with the same eigenvalues are not necessarily similar. Example 5 on Page 280 neatly explains the application of eigenvalues to 
dynamical systems. To get the expression for the initial state, find the scalar values that allow the eigenvectors to equate the initial state, thus giving you the full equation. 
All that was needed was the transformation matrix and the initial value of the system. Why the dynamical systems tend to a steady state value is that this steady state is a multiple 
of one of the eigenvectors for the matrix. The other eigenvectors drop out due to raising their $\lambda$ values to an increasing power - so they trend to zero. 
\subsection{Diagonolization}
If a given matrix is similar to a diagonol matrix, then this matrix is diagonolizable. For this to be true, the matrix has to have n (power of the matrix) linearly indpendent eigenvectors. 
Furthermore, the columns of P (used to determine similarity) must be n linearly independent eigenvectors of the matrix. For this case, the diagonol matrix has eigenvalues for A that 
correspond to the eigenvectors of P. In summary, there needs to be enough eigenvectors to form a basis of A. 
There are four steps for finding if a given matrix is diagonolizable:
\begin{itemize}
    \item Find the eigenvalues of the given matrix by utilizing the characteristic equation and solving for the roots including multiplicities
    \item Find n linearly independent eigenvectors of the matrix by using the vector equation and the found eigenvalues
    \item Construct P from any combination of the eigenvectors as columns
    \item Construct D from the same order of eigenvalues in a diagonol matrix
\end{itemize}
If there are n eigenvalues for an \textit{n x n} matrix, it is diagonolizable, but this is not a requirement as multiplicities can factor in as well. The kth power of the matrix is the 
Diagonolization of the matrix to the kth power with the similarity matrices on each side. The transformation matrix and the eigenvectors can be multiplied in order to determine the 
product of the eigenvalues and the eigenvectors to isolate the eigenvalues. 
\subsection{Eigenvectors and Linear Transformations}
The linear transformation T onto \textbf{x} can be viewed as left multiplication by the matrix M on the basis of \textbf{x}. The matrix M is called the matrix for T relative to the bases 
$\beta$ and $\zeta$. In order to map this correctly, the $\beta$ mapping to $\zeta$ through T needs to be understood. If this is the case, M is found by combining the columns of 
the mappings of the $\beta$ components. If T is the identity matrix, then this reduces to a change of coordinates computation. When the dimensionality between the two spaces is the same 
and the coordinates for the bases are the same, then the mapping is referred to as the matrix for T relative to $\beta$. The images of the basis vectors are what the transformation for 
a given vector becomes. Basically for a polynomial, what the variable multiplied with the coefficients turns into after the transformation. From there, the $\beta$-coordinate vectors 
are how those transformations map into the original space based on the variables remaining. 
\newline
\newline
If $\beta$ is the basis for the space and formed by the P matrices from the diagonolization section, then D, the diagonol from that section, is the $\beta$-matrix. It is important to 
remember that any multiple of the eigenvectors through the free variable is considered a valid basis. Inversely, to find the beta matrix given the transformation matrix and the beta 
representations (P matrix), multiplying first by the inverse of P and then the transformation matrix, and then the P matrix yields the beta matrix. The Jordan form of the 
transformation matrix is a nondiagonol matrix that also is similar to the transformation matrix and thus also accurately describes the transformation. 

\end{document}