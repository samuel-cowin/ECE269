\documentclass[12pt]{article}
\usepackage{amsmath, bm, amssymb}
\usepackage{tikz, pgfplots}
\pgfplotsset{compat=1.17}
\usepackage[a4paper,bindingoffset=0.2in,left=1in,right=1in,top=1in,bottom=1in,footskip=.25in]{geometry}

\begin{document}
\setlength{\abovedisplayskip}{0pt}
\setlength{\belowdisplayskip}{0pt}
\setlength{\abovedisplayshortskip}{0pt}
\setlength{\belowdisplayshortskip}{0pt}

\title{Notes for ECE269 - Linear Algebra \\
\large Chapter 4}
\author{Sam Cowin}
\maketitle

\section{Linear Equations in Linear Algebra}
\section{Matrix Algebra}
\section{Determinants}
\section{Vector Spaces}
\section{Eigenvalues and Eigenvectors}
Eigenvalues and Eigenvectors appear in many systems, but concerning engineering, the appear in differential equations and continuous dynamical systems. These systems are another way to refer
to difference equations. The dynamical system describes the lienar transformations from one state to the next state, and the Eigenvalues and Eigenvectors help visualize and disect these 
transformations. 
\subsection{Eigenvectors and Eigenvalues}
An eigenvector of an \textit{n x n} matrix A is a nonzero vector \textbf{x} such that $A\mathbf{x}=\lambda\mathbf{x}$ for some scalar $\lambda$. A scalar $\lambda$ is called an eigenvalue of 
A if there is a nontrivial solution \textbf{x} of A\textbf{x}=$\lambda$\textbf{x}; such an \textbf{x} is called an eigenvector corresponding to $\lambda$. $\lambda$ is a eigenvalue of 
the \textit{n x n} matrix A if and only if there is a nontrivial solution to the equation $(A-\lambda I)\mathbf{x=0}$. The eigenspace is a subspace of the null space. Finding a basis for 
the eigenspace is the equivalent of solving the homogeneous equation just described and finding the vector equation corresponding to that solution. while this method works for finding the 
eigenvectors, the reduced echelon form does not showcase the eigenvalues. For there to be a nontrivial solution, to restate there needs to be linear dependence among the columns or free 
variables. One such case where eigenvalues can be found precisely is when you have a triangular matrix and the eigenvalues are the entries of the main diagonol. This is the 
case for the lower triangular matrices as well and repeats are treated as one eigenvalue. Zero can only be an eigenvalue of a matrix if that matrix is not invertible. If there is a set 
of eigenvectors that each correspond to a distinct eigenvalue, then these vectors are linearly independent. One simple way to solve the difference equations is to replace the 
previous state multiplied with the matrix A by the matrix A multiplied with the initial state eigenvector multiplied with eigenvalue of this state to the previous states power. The 
transformation matrix raised to any power multiplied with \textbf{x} is equivalent to the eigenvalue raised to the same power multiplied with \textbf{x}. Multiples of eigenvalues are 
eigenvalues for the same scalar multiple of the matrix that the original eigenvalue was an eigenvalue for. 
\subsection{The Characteristic Equation}
To find the eigenvalues for a given matrix, you must find $\lambda$ values that will make the matrix not invertible. This is done by finding $\lambda$ values that make the following true:
\newline
$$
det(A-\lambda I=0)
$$
\newline
Finding the roots of this equation yields the eigenvalues. This transformed the equation with two unknown in $\lambda$ and \textbf{x} into one with only one unknown in $\lambda$. To 
summarize previous findings, the determinant can be determined for higher order matrices by reducing to echelon form the matrix, and then multiplying -1 to the power of row interchanges 
needed by the pivot position values. If this value is 0, the matrix is not invertible. A scalar satisfying the above equation solves the characteristic equation. The degree of the 
characteristic equation is equivalent to the size of the matrix for an \textit{n x n} matrix. 
\newline
\newline
A matrix A is similar to the matrix B if there is an invertible matrix P such that $P^{-1}AP=B$ and this is referred to as the similarity transformation. If one matrix A and another B are 
similar, then they have the same characteristic polynomial and thus the same eigenvalues with the same multiplicities. Row operations normally change eigenvalues, so row equivalent 
matrices are not similar. Additionally, matrices with the same eigenvalues are not necessarily similar. Example 5 on Page 280 neatly explains the application of eigenvalues to 
dynamical systems. To get the expression for the initial state, find the scalar values that allow the eigenvectors to equate the initial state, thus giving you the full equation. 
All that was needed was the transformation matrix and the initial value of the system. Why the dynamical systems tend to a steady state value is that this steady state is a multiple 
of one of the eigenvectors for the matrix. The other eigenvectors drop out due to raising their $\lambda$ values to an increasing power - so they trend to zero. 
\subsection{Diagonolization}


\end{document}