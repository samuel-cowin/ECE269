\documentclass[12pt]{article}
\usepackage{amsmath, bm, amssymb}
\usepackage{tikz, pgfplots}
\pgfplotsset{compat=1.17}
\usepackage[a4paper,bindingoffset=0.2in,left=1in,right=1in,top=1in,bottom=1in,footskip=.25in]{geometry}

\begin{document}
\setlength{\abovedisplayskip}{0pt}
\setlength{\belowdisplayskip}{0pt}
\setlength{\abovedisplayshortskip}{0pt}
\setlength{\belowdisplayshortskip}{0pt}

\title{Notes for ECE269 - Linear Algebra \\
\large Chapter 4}
\author{Sam Cowin}
\maketitle

\section{Linear Equations in Linear Algebra}
\section{Matrix Algebra}
\section{Determinants}
\section{Vector Spaces}
\section{Eigenvalues and Eigenvectors}
Eigenvalues and Eigenvectors appear in many systems, but concerning engineering, the appear in differential equations and continuous dynamical systems. These systems are another way to refer
to difference equations. The dynamical system describes the lienar transformations from one state to the next state, and the Eigenvalues and Eigenvectors help visualize and disect these 
transformations. 
\subsection{Eigenvectors and Eigenvalues}
An eigenvector of an \textit{n x n} matrix A is a nonzero vector \textbf{x} such that $A\mathbf{x}=\lambda\mathbf{x}$ for some scalar $\lambda$. A scalar $\lambda$ is called an eigenvalue of 
A if there is a nontrivial solution \textbf{x} of A\textbf{x}=$\lambda$\textbf{x}; such an \textbf{x} is called an eigenvector corresponding to $\lambda$. $\lambda$ is a eigenvalue of 
the \textit{n x n} matrix A if and only if there is a nontrivial solution to the equation $(A-\lambda I)\mathbf{x=0}$. The eigenspace is a subspace of the null space. Finding a basis for 
the eigenspace is the equivalent of solving the homogeneous equation just described and finding the vector equation corresponding to that solution. while this method works for finding the 
eigenvectors, the reduced echelon form does not showcase the eigenvalues. For there to be a nontrivial solution, to restate there needs to be linear dependence among the columns or free 
variables. One such case where eigenvalues can be found precisely is when you have a triangular matrix and the eigenvalues are the entries of the main diagonol. This is the 
case for the lower triangular matrices as well and repeats are treated as one eigenvalue. Zero can only be an eigenvalue of a matrix if that matrix is not invertible. If there is a set 
of eigenvectors that each correspond to a distinct eigenvalue, then these vectors are linearly independent. One simple way to solve the difference equations is to replace the 
previous state multiplied with the matrix A by the matrix A multiplied with the initial state eigenvector multiplied with eigenvalue of this state to the previous states power. The 
transformation matrix raised to any power multiplied with \textbf{x} is equivalent to the eigenvalue raised to the same power multiplied with \textbf{x}. Multiples of eigenvalues are 
eigenvalues for the same scalar multiple of the matrix that the original eigenvalue was an eigenvalue for. 
\subsection{The Characteristic Equation}
To find the eigenvalues for a given matrix, you must find $\lambda$ values that will make the matrix not invertible. This is done by finding $\lambda$ values that make the following true:
\newline
$$
det(A-\lambda I=0)
$$
\newline
Finding the roots of this equation yields the eigenvalues. This transformed the equation with two unknown in $\lambda$ and \textbf{x} into one with only one unknown in $\lambda$. To 
summarize previous findings, the determinant can be determined for higher order matrices by reducing to echelon form the matrix, and then multiplying -1 to the power of row interchanges 
needed by the pivot position values. If this value is 0, the matrix is not invertible. A scalar satisfying the above equation solves the characteristic equation. The degree of the 
characteristic equation is equivalent to the size of the matrix for an \textit{n x n} matrix. 
\newline
\newline
A matrix A is similar to the matrix B if there is an invertible matrix P such that $P^{-1}AP=B$ and this is referred to as the similarity transformation. If one matrix A and another B are 
similar, then they have the same characteristic polynomial and thus the same eigenvalues with the same multiplicities. Row operations normally change eigenvalues, so row equivalent 
matrices are not similar. Additionally, matrices with the same eigenvalues are not necessarily similar. Example 5 on Page 280 neatly explains the application of eigenvalues to 
dynamical systems. To get the expression for the initial state, find the scalar values that allow the eigenvectors to equate the initial state, thus giving you the full equation. 
All that was needed was the transformation matrix and the initial value of the system. Why the dynamical systems tend to a steady state value is that this steady state is a multiple 
of one of the eigenvectors for the matrix. The other eigenvectors drop out due to raising their $\lambda$ values to an increasing power - so they trend to zero. 
\subsection{Diagonolization}
If a given matrix is similar to a diagonol matrix, then this matrix is diagonolizable. For this to be true, the matrix has to have n (power of the matrix) linearly indpendent eigenvectors. 
Furthermore, the columns of P (used to determine similarity) must be n linearly independent eigenvectors of the matrix. For this case, the diagonol matrix has eigenvalues for A that 
correspond to the eigenvectors of P. In summary, there needs to be enough eigenvectors to form a basis of A. 
There are four steps for finding if a given matrix is diagonolizable:
\begin{itemize}
    \item Find the eigenvalues of the given matrix by utilizing the characteristic equation and solving for the roots including multiplicities
    \item Find n linearly independent eigenvectors of the matrix by using the vector equation and the found eigenvalues
    \item Construct P from any combination of the eigenvectors as columns
    \item Construct D from the same order of eigenvalues in a diagonol matrix
\end{itemize}
If there are n eigenvalues for an \textit{n x n} matrix, it is diagonolizable, but this is not a requirement as multiplicities can factor in as well. The kth power of the matrix is the 
Diagonolization of the matrix to the kth power with the similarity matrices on each side. The transformation matrix and the eigenvectors can be multiplied in order to determine the 
product of the eigenvalues and the eigenvectors to isolate the eigenvalues. 
\subsection{Eigenvectors and Linear Transformations}
The linear transformation T onto \textbf{x} can be viewed as left multiplication by the matrix M on the basis of \textbf{x}. The matrix M is called the matrix for T relative to the bases 
$\beta$ and $\zeta$. In order to map this correctly, the $\beta$ mapping to $\zeta$ through T needs to be understood. If this is the case, M is found by combining the columns of 
the mappings of the $\beta$ components. If T is the identity matrix, then this reduces to a change of coordinates computation. When the dimensionality between the two spaces is the same 
and the coordinates for the bases are the same, then the mapping is referred to as the matrix for T relative to $\beta$. The images of the basis vectors are what the transformation for 
a given vector becomes. Basically for a polynomial, what the variable multiplied with the coefficients turns into after the transformation. From there, the $\beta$-coordinate vectors 
are how those transformations map into the original space based on the variables remaining. 
\newline
\newline
If $\beta$ is the basis for the space and formed by the P matrices from the diagonolization section, then D, the diagonol from that section, is the $\beta$-matrix. It is important to 
remember that any multiple of the eigenvectors through the free variable is considered a valid basis. Inversely, to find the beta matrix given the transformation matrix and the beta 
representations (P matrix), multiplying first by the inverse of P and then the transformation matrix, and then the P matrix yields the beta matrix. The Jordan form of the 
transformation matrix is a nondiagonol matrix that also is similar to the transformation matrix and thus also accurately describes the transformation. 
\subsection{Complex Eigenvalues}
Let A be a real \textit{2 x 2} matrix with a complex eigenvalue $\lambda = a-bi$ with b not being 0, and corresponding eigenvector \textbf{v}, then $A=PCP^{-1}$ where $P=\begin{bmatrix}
    Re{\mathbf{v}} & Im{\mathbf{v}}
\end{bmatrix}$
$$C=\begin{bmatrix}
    a & -b \\
    b & a
\end{bmatrix}$$
\subsection{Discrete Dynamical Systems}
When you have all of the eigenvalues of a system less than one, the origin (or point where the equation approaches) is called an attractor and the direction of greatest attraction is 
along the line through the zero vector and the eigenvector for the smaller eigenvalue. The opposite is true when both eigenvalues are greater than 1, being a repeller, and the 
direction of repellant is along the line through the zero vector and the eigenvector of the larger eigenvalue. Rejection and attraction are determined in the same way, and the saddle 
point is the location that movement is from. For matrices that are not diagonol, a change of variable and some algebra produce a matrix that utilizes only diagonols. A similar 
interpretation is made when there are complex eigenvalues and eigenvectors. If the absolute value of the eigenvalues is greater than 1, then the zero vector is a repeller and 
the iterates of the initial vector spiral outwards from the origin. The opposite is true if the absolute value of the eigenvalues is less than 1. Given a dynamical system 
and an initial value, solving for the coefficients is done with the initial state and eigenvectors, and from there the dynamical system is made from the eigenvalues, coefficients, 
and eigenvalues. 
\subsection{Applications to Differential Equations}
Eigenvalue-eigenvector pairs form solutions to the differential equation. Exponentials are necessary in this case as the derivative of the exponential will help to maintain the equality 
of the derivative equaling the matrix multiplied with the vector that is differentiated. Combining the eigenfunctions with the initial values works to yield the coefficients for the 
solutions. Complex eigenvalue-eigenvector pairs can be mapped into the real realm for solutions to the differential equations through the following expansions:
\newline
$$
e^{(a+bi)t}=e^{at}+e^{ibt}=e^{at}(cos(bt)+isin(bt))
$$
\newline
From there, the following equations show the solutions after the change in variable for any dimensionality:
\newline
$$
\mathbf{y_1(t)}=Re\mathbf{x_1(t)}=[(Re\mathbf{v})cos(bt)-(Im\mathbf{v})sin(bt)]e^{at}
$$
$$
\mathbf{y_2(t)}=Im\mathbf{x_1(t)}=[(Re\mathbf{v})sin(bt)+(Im\mathbf{v})cos(bt)]e^{at}
$$
\newline
Spiral points are another means to describe graphically what the solution space is doing and are a result of the trigonometric properties of the solutions from the complex eigenvalues. 
\subsection{Iterative Estimates for Eigenvalues}
An approximation for the dominant eigenvalue can be found in 5.8 on page 323 through Example 1. Multiplying the initial matrix with the initial state and scaling by the largest absolute 
value term yields the approximate largest eigenvalue and an eigenvector over time. The rate of convergence for this method depends on the ratio of the eigenvalues. For finding any eigenvalue, 
using the inverse power method with rough estimates for the eigenvalues can find you solutions to certain degrees of certainty. If there is no known estimate, zero can be used as an initial 
eigenvalue estimate to find the smallest eigenvalue. QR methods are more advanced and are used by Matlab in order to implement the same functionality as described here. 
\end{document}