\documentclass[12pt]{article}
\usepackage{amsmath, bm, amssymb}
\usepackage{tikz, pgfplots}
\pgfplotsset{compat=1.17}
\usepackage[a4paper,bindingoffset=0.2in,left=1in,right=1in,top=1in,bottom=1in,footskip=.25in]{geometry}

\begin{document}
\setlength{\abovedisplayskip}{0pt}
\setlength{\belowdisplayskip}{0pt}
\setlength{\abovedisplayshortskip}{0pt}
\setlength{\belowdisplayshortskip}{0pt}
    
\title{Notes for ECE269 - Linear Algebra \\
\large Chapter 2}
\author{Sam Cowin}
\maketitle

\section{Linear Equations in Linear Algebra}
\section{Matrix Algebra}
This section will outline the foundations of matrix operations that are used throughout many linear algebra applications. These include inversion, handlind multiple matrices %
, and partitioning matrices. These concepts will be shown through numerous examples demonstrating the utility.

\subsection{Matrix Operations}
If \textbf{A} is an \textit{m x n} matrix with m rows and n columns, then the scalar entry in the ith row and the jth column is denoted with $a_ij$. %
This denotion refers to beginning in the top left, being the ith entry down and the jth column vector. The main diagonol is formed when the i and j %
enumerations are equal. A diagonol matrix is a square matrix with the nondiagonol entries equating to zero. 
\newline
\newline
The sum of two matrices is only defined when they are the same size, as matrix addition is an extension of vector addition in that it is an item by item operation. %
Multiplying a matrix by a scalar is similar to vectors as well, in that each item in the matrix is multiplied by the scalar. Equality of theorems extending from this %
are proven by showing matrices on each side of the equation are of the same size and that their column vectors are equal. If \textbf{A} is an \textit{m x n} matrix and %
\textbf{B} is a \textit{n x p} matrix then the product of \textbf{AB} is an \textit{m x p} matrix as shown below:
\newline
$$
\mathbf{AB} = A\begin{bmatrix}
    \mathbf{b_1} & \mathbf{b_2} & \mathbf{\dots} & \mathbf{b_p}
\end{bmatrix}
$$
\newline
This multiplication of matrices corresponds to a composition of linear transformations. Alternatively, each column of \textbf{B} is used as a weight vector dot producted%
 to each column of \textbf{A}. For this reason the number of columns in \textbf{A} must match the rows in \textbf{B} or the dot product is not defined. The number of %
rows is maintained from \textbf{A} and the number of columns is maintained from \textbf{B}. This can be seen in Figure 1. 
\newline
\begin{figure}
\begin{align*}
    \overset{A}{\underset{\textcolor{blue}{3 x 5}}{
        \begin{array}{@{}c@{}}{
            \begin{bmatrix}
                * & * & * & * & * \\
                * & * & * & * & * \\
                * & * & * & * & * 
            \end{bmatrix}} \\ \\ \\
        \end{array}
    }}
    \overset{B}{\underset{\textcolor{blue}{5 x 2}}{
    \begin{bmatrix}
        * & * \\
        * & * \\
        * & * \\
        * & * \\
        * & *
    \end{bmatrix}}}
    \begin{array}{@{}c@{}}{
        =
    } \\ \\ \\  
    \end{array}
    \overset{AB}{\underset{\textcolor{blue}{3 x 2}}{
        \begin{array}{@{}c@{}}{
            \begin{bmatrix}
                * & * \\ 
                * & * \\ 
                * & * 
            \end{bmatrix}} \\ \\ \\
        \end{array}
    }}
\end{align*}
\caption{Equations showing the necessary dimensionality for proper matrix multiplication.}
\end{figure}
The entries in the final matrix, per row/column pair, correspond to the dot product of that row from the first matrix and the column from the second matrix. %
If AB = BA, then we state that these matrices commute with one another. Cancellation laws do not work for matrices (i.e. AB=AC does not mean B=C). % 
The product of A and B being the zero matrix does not mean that either is the zero matrix itself. The kth exponent of a matrix only translates to the multiple of that matrix %
k times for an \textit{n x n} matrix. Raising a matrix to the power of zero yields the identity matrix. The transpose of a matrix that is \textit{m x n} yields % 
a \textit{n x m} matrix whose columns are formed by the initial matrix's rows. The transpose of a product of matrices is equal to the product of their transposes %
in the reverse order. 
\subsection{The Inverse of a Matrix}
An \textit{n x n} matrix is invertible if there is an \textit{n x n} matrix that when multiplied to the original matrix and when the original matrix is multiplied with it %
(due to matrices not being communative) the result is the \textit{n x n} identity matrix. This second matrix is the unique inverse of the original. A matrix that is %
not invertible is referred to as a singular matrix. 
\newline
$$
A=\begin{bmatrix}
    a & b \\
    c & d
\end{bmatrix}. 
$$
\newline
\newline
If $ad-bc\neq0$, then \textbf{A} is invertible and the inverse is computed below. 
\newline
\newline
$$
\mathbf{A^{-1}}=\dfrac{1}{ad-bc}\begin{bmatrix}
    d & -b \\
    -c & a
\end{bmatrix}
$$
\newline
\newline
ad-bc is also referred to as the determinant in the above equation. If there exists an invertible matrix, then the inverse of this matrix multiplied with the result from %
the matrix equation is a solution to the original matrix equation. Much like transposes, the product of invertible matrices is invertible, and the inverse is the product %
of the inverses in reverse order. inversions and transposes can be flipped for equality of the same matrix. 
\newline
\newline
Elementary matrices can be used to describe the row operations used previously to reduce matrices into reduced echelon form. Below are the matrices and their descriptions. %
The first is an example of multiplying -4 times the first row and adding this to the third row, the second is swapping the first two rows, and the third is multiplying the %
third row by 5.
\newline
\newline
\newline
Addition of a scaled row to another row = $\begin{bmatrix}
    1 & 0 & 0 \\
    0 & 1 & 0 \\
    -4 & 0 & 1
\end{bmatrix}$ 
\newline
\newline
\newline
Swapping of two rows = $\begin{bmatrix}
    0 & 1 & 0 \\
    1 & 0 & 0 \\
    0 & 0 & 1
\end{bmatrix}$
\newline
\newline
\newline
Scaling of a single row = $\begin{bmatrix}
    1 & 0 & 0 \\
    0 & 1 & 0 \\
    0 & 0 & 5
\end{bmatrix}$
\newline
\newline
\newline
These elementary matrices are invertible in the same way their operations can be reversed. A matrix is only invertible if it is row equivalent to the identity matrix %
meaning that it can be row reduced to the identity matrix. The same operation that reduces a matrix to the identity matrix is used on the identity matrix in order to %
find the inverse of the original matrix that was deemed invertible. 
\newline
\newline
Algorithm for finding the inverse of matrix A: Row reduce the augmented matrix [\textbf{A I}]. If \textbf{A} is equivalent to \textbf{I}, then [\textbf{A I}] %
is row equivalent to [$\mathbf{I\medspace A^{-1}}$]. Otherwise, there is no inverse. Another view of this would be interpretting each column of the identity matrix %
separately. The value in this would be if you needed to only compute a couple of the inverted columns for a specific problem, in which case you would only do a portion. 
\subsection{Characterizations of Invertible Matrices}
Theorem 8 (Invertible Matrix Theorem) on page 114 of the text showcases equivalent statements for an \textit{n x n} matrix (square) meaning that all of the statements %
are true or they are all false. The statements range from proving independence of columns to certainty of a solution for the homogoeneous matrix. A linear transformation %
is only invertible if the standard matrix demonstrating this transformation is an invertible matrix, and if this is the case, there exists an inverse transformation %
that when applied to the original linear transformation yields the vector the transformations are applied to. The converse of the existence of a solution in the Invertible %
Matrix theorem is not that there isn't a solution for any b, just that the matrix is inconsistent for at least b. 
\subsection{Partitioned Matrices}
An extension of the previously used column interpretation of the matrices evaluated is partitioning matrices. The value here, is that implementing more specific horizontal %
and vertical divisions yields new ways to interpret matrices that are more relevant for a given application. An example can be seen below.
\newline
\newline
$$
A=\left[\begin{array}{c c c | c c | c}
    3 & 0 & -1 & 5 & 9 & -2 \\
    -5 & 2 & 4 & 0  & -3 & 1 \\
    \hline
    -8 & -6 & 3 & 1 & 7 & -4
\end{array}\right]=
\begin{bmatrix}
    \mathbf{A_11} & \mathbf{A_12} & \mathbf{A_13} \\
    \mathbf{A_21} & \mathbf{A_22} & \mathbf{A_23}
\end{bmatrix}
$$
\newline
\newline
One example where this notation can make sense is with a circuit board. The matrix diagonol (the entry concerned with itself) could refer to the actual IC's, while the %
other entries correspond to the relationships of the IC's to one another. Arithmetic operations between matrices with equivalent partitions are done on a block by block %
basis. This is useful for when you want to modify only a portion of the matrices and not the entire thing for computational speed. The operations can be done on the blocks %
and the original matrix can be modified based on these block designations. One thing to keep in mind, for matrix multiplication, and not element by element multiplication, %
The row parititons of the first matrix will need to match the column partitions of the second matrix. To be comformable for block multiplication, it is not enough to have %
the same block count. The individual blocks need to maintain the same number of components for the multiplication to make sense. Keeping in mind that matrix multiplication %
is not communative, extra attention needs to be made to interpret the row/column blocks correctly. The columns need to match the rows of the next matrix. When a transpose %
is applied to the matrix, this transpose is applied to all of the blocks within the matrix as well. %
Below is the column-row expansion of matrix multiplication.
\newline
\newline
\newline
If A is \textit{m x n} and B is \textit{n x p}, then 
$$
AB=\begin{bmatrix}
    col_1(A) & col_2(A) & \dots & col_n(A)
\end{bmatrix} \begin{bmatrix}
    [row_1(B) \\ row_2(B) \\ \vdots \\ row_3(B)]     
\end{bmatrix}=
$$
\newline
$$
col_1(A)row_1(B) + \dots + col_n(A)row_n(B)
$$
\newline
Invertible matrices can be found for block partitions by again realizing that the original partitioned matrix and the inverse need to equate to the identity matrix. %
Block diagonol matrices are invertible if each block on the diagonol is invertible as there are zero entries for the other blocks. 
\subsection{Matrix Facterizations}
Matrix facterization is opposite to matrix multiplication in the sense that multiplication was a synthesis of the information contained in the matrices while %
facterization recomposes the information in the matrices into more than one matrix. This is often completed in a preprocessing step - taking the information %
provided and putting it into an alternative format in order to make manipulating and understanding the information easier. One example of this is through %
\textbf{LU} Facterization shown below with the first matrix being the L (\textit{m x m} unit lower triangular matrix) and the right being U (\textit{m x n} echelon %
form of A). %
\newline
$$
A=\begin{bmatrix}
    1 & 0 & 0 & 0 \\
    * & 1 & 0 & 0 \\
    * & * & 1 & 0 \\
    * & * & * & 1
\end{bmatrix}\begin{bmatrix}
    \scriptstyle \blacksquare & * & * & * & * \\
    0 & \scriptstyle \blacksquare & * & * & * \\
    0 & 0 & 0 & \scriptstyle \blacksquare & * \\
    0 & 0 & 0 & 0 & 0 \\
\end{bmatrix}
$$
\newline
By formulating the original matrix A in this way, this allows consecutive, simpler operations to carry out in order to solve the overall system. This is due to %
being able to write $\mathbf{Ax=L(Ux)=b}$ and substituting y for Ux. This facterization improves the efficiency on solving the initial problem by reducing the number %
necessary operations given that you know L and U. (Side note. Row reduction to solve an equation \textit{is} inversion). To form the LU facterization, reduce A to %
an echelon form U by sequence of row replacements, if possible, then place entries in L so the same operations reduce L to the identity matrix. Another way of stating %
this, as you reduce A to U or the echelon form, take the columns of the matrix being reduced and enter then into L and divide the column by the pivot. 

\end{document}