\documentclass[12pt]{article}
\usepackage{amsmath, bm, lipsum}
\usepackage{tikz, pgfplots}
\pgfplotsset{compat=1.17}
\usepackage[a4paper,bindingoffset=0.2in,left=1in,right=1in,top=1in,bottom=1in,footskip=.25in]{geometry}

\begin{document}
\setlength{\abovedisplayskip}{0pt}
\setlength{\belowdisplayskip}{0pt}
\setlength{\abovedisplayshortskip}{0pt}
\setlength{\belowdisplayshortskip}{0pt}
    
\title{Notes for ECE269 - Linear Algebra \\
\large Chapter 2}
\author{Sam Cowin}
\maketitle

\section{Linear Equations in Linear Algebra}
\section{Matrix Algebra}
This section will outline the foundations of matrix operations that are used throughout many linear algebra applications. These include inversion, handlind multiple matrices %
, and partitioning matrices. These concepts will be shown through numerous examples demonstrating the utility.

\subsection{Matrix Operations}
If \textbf{A} is an \textit{m x n} matrix with m rows and n columns, then the scalar entry in the ith row and the jth column is denoted with $a_ij$. %
This denotion refers to beginning in the top left, being the ith entry down and the jth column vector. The main diagonol is formed when the i and j %
enumerations are equal. A diagonol matrix is a square matrix with the nondiagonol entries equating to zero. 
\newline
\newline
The sum of two matrices is only defined when they are the same size, as matrix addition is an extension of vector addition in that it is an item by item operation. %
Multiplying a matrix by a scalar is similar to vectors as well, in that each item in the matrix is multiplied by the scalar. Equality of theorems extending from this %
are proven by showing matrices on each side of the equation are of the same size and that their column vectors are equal. If \textbf{A} is an \textit{m x n} matrix and %
\textbf{B} is a \textit{n x p} matrix then the product of \textbf{AB} is an \textit{m x p} matrix as shown below:
\newline
$$
\mathbf{AB} = A\begin{bmatrix}
    \mathbf{b_1} & \mathbf{b_2} & \mathbf{\dots} & \mathbf{b_p}
\end{bmatrix}
$$
\newline
This multiplication of matrices corresponds to a composition of linear transformations. Alternatively, each column of \textbf{B} is used as a weight vector dot producted%
 to each column of \textbf{A}. For this reason the number of columns in \textbf{A} must match the rows in \textbf{B} or the dot product is not defined. The number of %
rows is maintained from \textbf{A} and the number of columns is maintained from \textbf{B}. This can be seen in Figure 1. 
\newline
\begin{figure}
\begin{align*}
    \overset{A}{\underset{\textcolor{blue}{3 x 5}}{
        \begin{array}{@{}c@{}}{
            \begin{bmatrix}
                * & * & * & * & * \\
                * & * & * & * & * \\
                * & * & * & * & * 
            \end{bmatrix}} \\ \\ \\
        \end{array}
    }}
    \overset{B}{\underset{\textcolor{blue}{5 x 2}}{
    \begin{bmatrix}
        * & * \\
        * & * \\
        * & * \\
        * & * \\
        * & *
    \end{bmatrix}}}
    \begin{array}{@{}c@{}}{
        =
    } \\ \\ \\  
    \end{array}
    \overset{AB}{\underset{\textcolor{blue}{3 x 2}}{
        \begin{array}{@{}c@{}}{
            \begin{bmatrix}
                * & * \\ 
                * & * \\ 
                * & * 
            \end{bmatrix}} \\ \\ \\
        \end{array}
    }}
\end{align*}
\caption{Equations showing the necessary dimensionality for proper matrix multiplication.}
\end{figure}
The entries in the final matrix, per row/column pair, correspond to the dot product of that row from the first matrix and the column from the second matrix. %
If AB = BA, then we state that these matrices commute with one another. Cancellation laws do not work for matrices (i.e. AB=AC does not mean B=C). % 
The product of A and B being the zero matrix does not mean that either is the zero matrix itself. The kth exponent of a matrix only translates to the multiple of that matrix %
k times for an \textit{n x n} matrix. Raising a matrix to the power of zero yields the identity matrix. The transpose of a matrix that is \textit{m x n} yields % 
a \textit{n x m} matrix whose columns are formed by the initial matrix's rows. The transpose of a product of matrices is equal to the product of their transposes %
in the reverse order. 
\subsection{The Inverse of a Matrix}
An \textit{n x n} matrix is invertible if there is an \textit{n x n} matrix that when multiplied to the original matrix and when the original matrix is multiplied with it %
(due to matrices not being communative) the result is the \textit{n x n} identity matrix. This second matrix is the unique inverse of the original. A matrix that is %
not invertible is referred to as a singular matrix. 
\newline
$$
A=\begin{bmatrix}
    a & b \\
    c & d
\end{bmatrix}. 
$$
\newline
\newline
If $ad-bc\neq0$, then \textbf{A} is invertible and the inverse is computed below. 
\newline
\newline
$$
\mathbf{A^{-1}}=\dfrac{1}{ad-bc}\begin{bmatrix}
    d & -b \\
    -c & a
\end{bmatrix}
$$
\newline
\newline
ad-bc is also referred to as the determinant in the above equation. If there exists an invertible matrix, then the inverse of this matrix multiplied with the result from %
the matrix equation is a solution to the original matrix equation. Much like transposes, the product of invertible matrices is invertible, and the inverse is the product %
of the inverses in reverse order. inversions and transposes can be flipped for equality of the same matrix. 
\newline
\newline
Elementary matrices can be used to describe the row operations used previously to reduce matrices into reduced echelon form. Below are the matrices and their descriptions. %
The first is an example of multiplying -4 times the first row and adding this to the third row, the second is swapping the first two rows, and the third is multiplying the %
third row by 5.
\newline
\newline
\newline
Addition of a scaled row to another row = $\begin{bmatrix}
    1 & 0 & 0 \\
    0 & 1 & 0 \\
    -4 & 0 & 1
\end{bmatrix}$ 
\newline
\newline
\newline
Swapping of two rows = $\begin{bmatrix}
    0 & 1 & 0 \\
    1 & 0 & 0 \\
    0 & 0 & 1
\end{bmatrix}$
\newline
\newline
\newline
Scaling of a single row = $\begin{bmatrix}
    1 & 0 & 0 \\
    0 & 1 & 0 \\
    0 & 0 & 5
\end{bmatrix}$
\newline
\newline
\newline
These elementary matrices are invertible in the same way their operations can be reversed. A matrix is only invertible if it is row equivalent to the identity matrix %
meaning that it can be row reduced to the identity matrix. The same operation that reduces a matrix to the identity matrix is used on the identity matrix in order to %
find the inverse of the original matrix that was deemed invertible. 
\newline
\newline
Algorithm for finding the inverse of matrix A: Row reduce the augmented matrix [\textbf{A I}]. If \textbf{A} is equivalent to \textbf{I}, then [\textbf{A I}] %
is row equivalent to [$\mathbf{I\medspace A^{-1}}$]. Otherwise, there is no inverse. Another view of this would be interpretting each column of the identity matrix %
separately. The value in this would be if you needed to only compute a couple of the inverted columns for a specific problem, in which case you would only do a portion. 
\subsection{Characterizations of Invertible Matrices}


\end{document}