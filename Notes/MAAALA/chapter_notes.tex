\documentclass[12pt]{article}
\usepackage{amsmath, bm, amssymb}
\usepackage{tikz, pgfplots}
\pgfplotsset{compat=1.17}
\newcommand{\nhat}[1]{\hat#1}  

\begin{document}
\title{ECE269 - Linear Algebra \\
\large Professor: Dr. Piya Pal \\
\large Textbook: Matrix Analysis and Applied Linear Algebra}
\author{Notes by: Sam Cowin}
\maketitle

\section{Vector Spaces}
\subsection{Spaces and Subspaces}
While linear algebra can be extended well to matrices, it is limiting to believe that matrices are the only mechanism in which to apply linear algebraic principles. There are 
many other mediums such as polynomials and functions that have similar properties to matrices that linear algebra can also be applied to. The general discussion of linear 
algebra thus refers to vector spaces. 
\newline
\newline
A vector space involves four things (a nonempty set of vectors V, a scalar field F such as real numbers, vector addition, and scalar multiplication) and abides by the following 
axioms:
\begin{itemize}
    \item $\mathbf{x+y}\in V$ for all $\mathbf{x, y}\in V$. This is the closure property of vector addition.
    \item $(\mathbf{x+y})+\mathbf{z}=\mathbf{x}+(\mathbf{y+z})$ for all $\mathbf{x, y, z}\in V$. This is associativity of vector addition.
    \item $\mathbf{x+y}=\mathbf{y+x}$ for all $\mathbf{x, y}\in V$. This is commutativity of vector addition. 
    \item There is an element $\mathbf{0}$ such that $\mathbf{x+0=x}$ for every $\mathbf{x}\in V$. This is the vector addition additive identity.
    \item There is an element $\mathbf{-x}$ such that $\mathbf{x+(-x)=0}$ for every $\mathbf{x}\in V$. This is the vector addition additive inverse. 
    \item $\alpha\mathbf{x}\in V$ for all $\alpha\in F$ and $\mathbf{x}\in V$. This is the closure property of scalar multiplication.
    \item $\alpha\beta\mathbf{(x)}=\alpha\mathbf{(\beta x)}$ for all $\alpha, \beta\in F$ and $\mathbf{x}\in V$. This is associativity of scalar multiplication.
    \item $\alpha\mathbf{(x+y)}=\alpha\mathbf{x}+\alpha\mathbf{y}$ for all $\alpha\in F$ and $\mathbf{x, y}\in V$. This is the distributive property of scalar multiplication.
    \item $(\alpha+\beta)\mathbf{x}=\alpha\mathbf{x}+\beta\mathbf{x}$ for all $\alpha\beta\in F$ and $\mathbf{x}\in V$. This is also the distributive property of scalar 
    multiplication.
    \item $1\mathbf{x}=x$ for every $\mathbf{x}\in V$. This is the scalar multiplication multiplicative identity. 
\end{itemize}
As an example of the earlier generalization from matrices, functions can be applied in the same way as the scalar multiplications. Functions can be thought of as 
transformations. 
\newline
\newline
If there is a subset S for the vector space V over F, and S operates through the same axioms as defined previously for the vector space, then S is a subspace of V if :
\begin{itemize}
    \item $\mathbf{x,y\in S} \Rightarrow \mathbf{x+y\in S}$
    \item $\mathbf{x\in S} \Rightarrow \alpha\mathbf{x\in S}$ for all $\alpha\in F$
\end{itemize}
For the set containing only the zero vector, this subspace is referred to as the trivial subspace. The only subspaces that are proper are either this trivial subspace or 
straight lines through the origin. As the dimensionality of the vector space increases, so do the proper subspace options. Another interpretation of subspace is flat surfaces 
through the origin, or the trivial solution. 
\newline
\newline
For a given set of vectors, $S={\mathbf{v_1, v_2, \dotsc, v_r}}$ the subspace $span(S)={\alpha\mathbf{v_1}, \alpha\mathbf{v_2}, \dotsc, \alpha\mathbf{v_r}}$ generated by 
using only linear combinations of S is the space spanned by S. If there is a vector space V which equals the space spanned by S, S spans V. Subspaces can be added together 
to form another subspace and the sums of their respective spans, if they span their own subspaces, spans the added together subspace. 
\newline
\newline
A helpful observation is that for a given set of vectors S from a subspace V, S spans V if and only if for each vector in the subspace, there is a column vector that when 
multiplied with the column matrix from S, you obtain the vector in the subspace. This is deduced from the rank (number of pivots, pivotal columns, or nonzero rows) and 
the fact that for A$\mathbf{x=b}$ to be consistent, the rank of A needs to equal the rank of A|b. 
\subsection{Four Fundamental Subspaces}
\subsection{Linear Independence}
\subsection{Basis and Dimension}
\subsection{More about Rank}
\subsection{Classical Least-Squares}
\subsection{Linear Transformations}
\subsection{Change of Basis and Similarity}
\subsection{Invariant Subspaces}

\end{document}