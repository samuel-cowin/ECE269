\documentclass[12pt]{article}
\usepackage{amsmath, bm, amssymb}
\usepackage{tikz, pgfplots}
\pgfplotsset{compat=1.17}
\usepackage[a4paper,bindingoffset=0.2in,left=1in,right=1in,top=1in,bottom=1in,footskip=.25in]{geometry}
\newcommand\norm[1]{\left\lVert#1\right\rVert}

\begin{document}
\setlength{\abovedisplayskip}{0pt}
\setlength{\belowdisplayskip}{0pt}
\setlength{\abovedisplayshortskip}{0pt}
\setlength{\belowdisplayshortskip}{0pt}

\title{Notes for ECE269 - Linear Algebra \\
\large Chapter 6}
\author{Sam Cowin}
\maketitle

\section{Linear Equations in Linear Algebra}
\section{Matrix Algebra}
\section{Determinants}
\section{Vector Spaces}
\section{Eigenvalues and Eigenvectors}
\section{Orthoginality and Least Squares}
\section{Symmetric Matrices and Quadratic Forms}
Being able to interpret multiple streams of media in order to understand a centralized matrix is the desire with this chapter. The example provided is that in which there are 
multiple sattelites that are reading in images of the planet and these are combined in order to have a more hollistic picture of the scence than from just one camera. This can be 
extended into other fields as well such as computer vision and image processing. The diagonolization and the orthoginality from the previous chapters will be utilized to 
resolve these topics. 
\subsection{Diagonolization of Symmetric Matrices}
A symmetric matrix is one in which the transpose of the matrix is equal to the matrix itself. A matrix of this variety is necessarily square. The entries along the main diagonol can be 
any value, but the other values occur in pairs across the main diagonol. To repeat the diagonolization process here, you find the Eigenvalues and Eigenvectors, and the Eigenvalues are 
the main diagonol in one factorization of the matrix and the other matrix are the corresponding Eigenvectors as columns. If there Eigenvectors are proven to be orthogonal, then normalizing 
them for the orthonormal basis will help in calculations. The main difference when there is a orthogonal matrix in the similarity equation is that the inverse can be replaced with the 
transpose. The reason the Eigenvectors are orthogonal is that when the original matrix A is symmetric, then any two Eigenvectors from different eigenspaces are necessarily orthogonal. 
A given matrix is orthogonally diagonolizable if and only if the matrix is symmetric. 
\newline
\newline
When you find Eigenvectors are not orthogonal, you can utilize the projection of the Eigenvectors onto one of the others through the Gram-Schmidt process in order to obtain an orthogonal 
basis for the eigenspace. The Spectral Theorem related to the spectrum (set of eigenvalues) of a given matrix is listed on page 399 of the text. This outlines that for a symmetric matrix 
there are n Eigenvalues including multiplicities, each Eigenvalue corresponds to a distinct eigenspace that is orthogonal to the others, and the multiplicity of the Eigenvalues 
correspond to the dimensionality of the eigenspace. 
\subsection{Quadratic Forms}
The quadratic form is described as amatrix for each value of a given vector \textbf{x} being the transpose of the vector, multiplied with a symmetric matrix A, then multiplied with 
the vector itself. The matrix A is the matrix of the quadratic form. The matrix diagonols are the coefficients for the given variables and the cross-product terms in the equation are 
the remainder of the symmetric matrix A. There is a change of variable where you take the unit eigenvectors corresponding to each eigenspace of the symmetric matrix and you diagonolize 
with these components. The symmetric matrix is then replaced by the similarity matrix of the transpose of the unit eigenvectors matrix, the original matrix, and the eigenvectors matrix 
(or just the diagonol matrix from the diagonolization). What this enables you to do is eliminate the cross-product terms in the equation. The eigenvector matrix is also referred to as 
the principal axes of the quadratic form. These are needed to transform the values of the original variables to the new variables to make sense of the new equation through diagonolization. 
\newline
\newline
The solutions to the quadratic form are either a circle, a hyperbola, two intersecting lines, or a point. The standard position of these solution sets are about the origin and are not 
scaled or shifted out of place. What the cross-product terms do is move the solution sets out of the standard position. The change of variable through the principal axes is finding the 
axes that put the solution set back into a standard position. The quadratic form is positive definite if all of the values are greater than zero for values not zero, negative definite 
follows similarly, and indefinite is when the form does not have only positive or negative values. These relationships are directly related to the eigenvalues. 
\subsection{Constrained Optimization} 
The change of variable and the normalization for the unit Eigenvectors enables solving an optimization problem in finding the minimum and maximum values for a given quadratic form. 
When you have the unit eigenvectors, the minimum and maximum eigenvalues correspond to the minimum and the maximum of the quadratic form. If the transpose of the solution with the unit 
vector is zero, then the second largest (smallest) eigenvalue is chosen in place for the max (min). This continues for k-1 values that are not being utilized in the quadratic form. The 
kth unit eigenvector for the kth eigenvalue is the point of optimization. A way to change the variables of a non-unit quadratic form is to divide through to get 1 as the result, and then 
to change the squared variables and coefficients to be the new variables. 
\subsection{The Singular Value Decomposition}
The singular values for a given matrix are the square roots of the eigenvalues of the transpose of the matrix multiplied with itself. This holds for the unit vectors, and the singular values 
are equivalent to the length of the matrix multiplied with the eigenvector of the tranpose square. The second singular value or eigenvalue can also be interpretted as the maximum over all 
unit vectors that are orthogonal to the first eigenvector. The matrix/eigenvector products of the transpose square forms an orthogonal basis for the original matrix column space and 
the rank of such a matrix is equal to the number of components in the basis - excluding zero singular values. The singular values combined with zeros in a matrix to form a sized matrix equal 
to the original matrix is the singular value decomposition of the matrix as long as there are left singular vectors and right singular vectors that are orthogonal to the SVD with the squared 
dimensions of the original matrix. The right singular vectors are formed by the unit eigenvectors of the transpose square of the matrix. The normalized unit vectors of the singular values 
form the left singular vectors. In the case when the left or right singular vectors  do not complete the orthonormal basis for the dimensionality they need, extending the basis by finding 
vectors orthogonal to the vector already found and normalizing is the way to go. Orthoginality can be found by taking the first vector as an equation and finding the basis for the solution 
set. A matrix is invertible is it has n nonzero singluar values. 
\subsection{Applications to Image Processing and Statistics}
The diagonol entries in the covariance matrix are just the variance of the components on the diagonol. The other components of the covariance matrix determine the correlation bewteen 
variables. The goal of principal component analysis is to find a matrix transformation that makes the covariance matrix into another one that is completely uncorrelated and that lists the 
individual variances in decreasing order. The principal components of the data are the unit eigenvectors of the covariance matrix and are lienar combinations of the variables. 
The eigenvalues then correspond to the variance of the individual variables. The percentage of the variance encaptured in each principal component describes the importance of that value 
and if the discrepency is large enough, can be used to reduce the dimensionality of the data. 
\end{document}